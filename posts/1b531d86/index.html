<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/my-favicon-16x16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"rovo98.github.io","root":"/","images":"/images","scheme":"Mist","version":"8.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="文献翻译：文本分类字符级别卷积网络 authors: Xiang Zhang, Junbo Zhao, Yann LeCun 原文: Character-level Convolutional Networks for Text Classification   Abstract 本文提供了使用字符级卷积网络（ConvNets）进行文本分类的实证研究。我们构建了几个大型数据集，以表明字符级卷积网"><meta property="og:type" content="article"><meta property="og:title" content="Character-level Convolutional Networks for Text Classification"><meta property="og:url" content="http://rovo98.github.io/posts/1b531d86/index.html"><meta property="og:site_name" content="rovo98&#39;s Blog"><meta property="og:description" content="文献翻译：文本分类字符级别卷积网络 authors: Xiang Zhang, Junbo Zhao, Yann LeCun 原文: Character-level Convolutional Networks for Text Classification   Abstract 本文提供了使用字符级卷积网络（ConvNets）进行文本分类的实证研究。我们构建了几个大型数据集，以表明字符级卷积网"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/alphabet.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/figure-1.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/table-1.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/table-2.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/figure-2.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/table-3.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/table-4.png"><meta property="og:image" content="http://rovo98.github.io/posts/1b531d86/figure-3.png"><meta property="article:published_time" content="2020-01-12T16:00:00.000Z"><meta property="article:modified_time" content="2021-09-09T14:11:36.760Z"><meta property="article:author" content="rovo98"><meta property="article:tag" content="paper translation"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://rovo98.github.io/posts/1b531d86/alphabet.png"><link rel="canonical" href="http://rovo98.github.io/posts/1b531d86/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://rovo98.github.io/posts/1b531d86/","path":"posts/1b531d86/","title":"Character-level Convolutional Networks for Text Classification"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>Character-level Convolutional Networks for Text Classification | rovo98's Blog</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="alternate" href="/atom.xml" title="rovo98's Blog" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">rovo98's Blog</h1><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">Leave your comfort zone!</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">46</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">19</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">41</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-character-level-convolutional-networks"><span class="nav-number">3.</span> <span class="nav-text">2. Character-level Convolutional Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#21-key-modules"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Key Modules</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-character-quantization"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 Character quantization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-model-design"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 Model Design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#24-data-augmentation-using-thesaurus"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 Data Augmentation using Thesaurus</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-comparison-models"><span class="nav-number">4.</span> <span class="nav-text">3. Comparison Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#31-deep-learning-methods"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Deep Learning Methods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32-choice-of-alphabet"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Choice of Alphabet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-large-scale-datasets-and-results"><span class="nav-number">5.</span> <span class="nav-text">4. Large-scale Datasets and Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-discussion"><span class="nav-number">6.</span> <span class="nav-text">5. Discussion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-conclusion-and-outlook"><span class="nav-number">7.</span> <span class="nav-text">6. Conclusion and Outlook</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#acknowledgement"><span class="nav-number">8.</span> <span class="nav-text">Acknowledgement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">9.</span> <span class="nav-text">References</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="rovo98" src="/images/personal-logo.jpg"><p class="site-author-name" itemprop="name">rovo98</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap site-overview-item animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">46</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">19</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">41</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author site-overview-item animated"><span class="links-of-author-item"><a href="https://github.com/rovo98" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;rovo98" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a></span></div><div class="cc-license site-overview-item animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll site-overview-item animated"><div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://dclunatic.github.io/" title="https:&#x2F;&#x2F;dcLunatic.github.io" rel="noopener" target="_blank">dcLunatic</a></li><li class="links-of-blogroll-item"><a href="https://adj325.github.io/" title="https:&#x2F;&#x2F;adj325.github.io" rel="noopener" target="_blank">Adj325</a></li><li class="links-of-blogroll-item"><a href="https://rovo98.github.io/leetcode-solutions" title="https:&#x2F;&#x2F;rovo98.github.io&#x2F;leetcode-solutions">rovo98's Leetcode Adventure</a></li></ul></div></div></div></div></div></aside><div class="sidebar-dimmer"></div></header><div class="back-to-top" role="button" aria-label="Back to top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en"><link itemprop="mainEntityOfPage" href="http://rovo98.github.io/posts/1b531d86/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/personal-logo.jpg"><meta itemprop="name" content="rovo98"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="rovo98's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Character-level Convolutional Networks for Text Classification</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2020-01-13 00:00:00" itemprop="dateCreated datePublished" datetime="2020-01-13T00:00:00+08:00">2020-01-13</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/dissertation/" itemprop="url" rel="index"><span itemprop="name">dissertation</span></a> </span></span><span id="/posts/1b531d86/" class="post-meta-item leancloud_visitors" data-flag-title="Character-level Convolutional Networks for Text Classification" title="Views"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span> </span><span class="post-meta-item" title="Views" id="busuanzi_container_page_pv"><span class="post-meta-item-icon"><i class="far fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span id="busuanzi_value_page_pv"></span></span></div></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>文献翻译：文本分类字符级别卷积网络<br>authors: Xiang Zhang, Junbo Zhao, Yann LeCun<br>原文: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1509.01626">Character-level Convolutional Networks for Text Classification</a></p></blockquote><h2 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h2><p>本文提供了使用字符级卷积网络（ConvNets）进行文本分类的实证研究。我们构建了几个大型数据集，以表明字符级卷积网络可以达到最新水平或竞争结果。可以与传统模型（例如词袋BoW，n-grams 其 TFIDF 变体）以及深度学习模型（例如基于单词的 ConvNets 和递归神经网络）进行比较。</p><span id="more"></span><h2 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h2><p>文本分类是自然语言处理的经典研究问题，其中需要为自由文本文档分配预定义的类别。文本分类研究的范围从设计最佳特征到选择最佳的机器学习分类器。迄今为止，几乎所有的文本分类技术都是基于单词的，其中一些有序的单词组合（例如 n-gram）的简单统计通常表现最佳<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。</p><p>另一方面，许多研究人员发现卷积网络（ConvNets）<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup><sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>非常适合用于从原始信息中提取信息，因而卷积网络从计算机视觉领域被扩展到语音识别等领域。特别地，深度学习研究中早期使用的时延网络本质上是对序列数据进行建模的卷积网络。<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup><sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>。</p><p>在本文中，我们探索将文本视为一种字符级别的原始信号，并对其应用时间（一维）卷积网络 ConvNets。本文仅使用分类任务作为例证，说明 ConvNets 理解文本的能力。从以往的研究，我们知道 ConvNets 通常需要大规模的数据集才能很好地工作，因此我们也构建了几个这样的数据集。并提供了传统模型和其他深度学习模型的一组比较。</p><p>已有文献探索了将卷积网络应用于文本分类或整个自然语言处理中，并已经证明，ConvNets 可以直接应用于分布<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>或离散<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>的词嵌入（Word embedding）中，而无需了解语言的语法或语义结构。这些方法已被证明与传统模型相比具有竞争力。</p><p>目前已有一些使用字符级别特征进行自然语言处理的相关研究工作。其中包含使用带有线性分类器的字符级别 n-gram<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>,以及将字符级别特征合并到 ConvNets<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup><sup class="footnote-ref"><a href="#fn11" id="fnref11">[11]</a></sup>。这些 ConvNet 方法使用单词作为基础，在单词<sup class="footnote-ref"><a href="#fn10" id="fnref10:1">[10:1]</a></sup>或单词 n-gram<sup class="footnote-ref"><a href="#fn11" id="fnref11:1">[11:1]</a></sup> 的分布式表示上提取字符级别特征，对词性标记和信息检索进行改进。</p><p>本文是首次仅在字符级别上应用 ConvNets 的文章。我们证明，在大规模数据集上进行训练时，除了以前的研究得出的，即 ConvNets 不需要语言的语法或语义结构方面的知识外，深层 ConvNets 也是不需要单词的相关的知识的。这种工程上的简化对于可以适用于不同语言的单个系统而言至关重要，因为字符始终是文本必要的构成元素，不管是否将文本分割成单词。仅处理字符还具有以下优点：可以自然地学习诸如拼写错误和表情符号之类的异常字符组合。</p><h2 id="2-character-level-convolutional-networks"><a class="markdownIt-Anchor" href="#2-character-level-convolutional-networks"></a> 2. Character-level Convolutional Networks</h2><p>在本节中，我们经介绍文本分类的字符级别 ConvNets 的设计。该设计是模块化的，并通过反向传播<sup class="footnote-ref"><a href="#fn12" id="fnref12">[12]</a></sup>计算梯度来执行优化。</p><h3 id="21-key-modules"><a class="markdownIt-Anchor" href="#21-key-modules"></a> 2.1 Key Modules</h3><p>主要组件是时间卷积模块，它仅计算一维卷积。假设有一个离散输入函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>l</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g(x)\in[1,l]\to \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span>和一个离散核函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f(x)\in[1,k]\to\mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span>。步长（stride）为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathdefault">d</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>之间的卷积<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi>l</mi><mo>−</mo><mi>k</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>d</mi><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">h(y)\in[1,[(l-k)/d]+1]\to\mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span> 定义如下</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>g</mi><mo stretchy="false">(</mo><mi>y</mi><mo>⋅</mo><mi>d</mi><mo>−</mo><mi>x</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h(y) = \sum^k_{x=1}f(x)\cdot g(y\cdot d-x+c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.1032260000000003em;vertical-align:-1.267113em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8361130000000003em"><span style="top:-1.882887em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.267113em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span></span></p><p>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>=</mo><mi>k</mi><mo>−</mo><mi>d</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">c=k-d+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span>是一个偏置常数。就像在计算机视觉上的传统卷积网络一样，该模块由一组这样的内核函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>m</mi><mspace width="1em"><mi>a</mi><mi>n</mi><mi>d</mi><mspace width="1em"><mi>j</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{ij}(x)(i=1,2,...,m\quad and\quad j=1,2,...,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.10764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord mathdefault">i</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right:1em"></span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:1em"></span><span class="mord mathdefault" style="margin-right:.05724em">j</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span> 来进行参数化，通常称为权重（weights），以及一组输入 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_i(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 和输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_j(y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span></span></span></span>。我们称每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 为输入特征，每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>为输出特征，用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">m</span></span></span></span> 表示输入特征的大小，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">n</span></span></span></span> 表示输出特征的大小。通过在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g_i(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{ij}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:-.10764em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 之间卷积在 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.65952em;vertical-align:0"></span><span class="mord mathdefault">i</span></span></span></span> 上的总和来表示输出 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_j(y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span></span></span></span>。</p><p>帮助我们训练更深层模型的一个关键模块是时间最大池化（temporal max-pooling，一维最大池化）。它是计算机视觉<sup class="footnote-ref"><a href="#fn13" id="fnref13">[13]</a></sup>中使用的最大池化模块的一维版本。给定一个离散输入函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mi>l</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g(x)\in [1,l]\to \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span>， <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> 的 max-pooling 函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi>l</mi><mo>−</mo><mi>k</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>d</mi><mo stretchy="false">]</mo><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">h(y)\in [1,[(l-k)/d]+1]\to \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mopen">[</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathdefault">d</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68889em;vertical-align:0"></span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span> 定义如下</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><msubsup><mi>x</mi><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mi>g</mi><mo stretchy="false">(</mo><mi>y</mi><mo>⋅</mo><mi>d</mi><mo>−</mo><mi>x</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h(y) = max_{x=1}^k g(y\cdot d -x +c)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.1491079999999998em;vertical-align:-.25em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8991079999999998em"><span style="top:-2.4530000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.247em"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:.03588em">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>=</mo><mi>k</mi><mo>−</mo><mi>d</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">c=k-d+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault" style="margin-right:.03148em">k</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 是一个偏置常数。这样的池化模块能使我们训练超过 6 层的 ConvNets 模型，而其他的层均会失败，<sup class="footnote-ref"><a href="#fn14" id="fnref14">[14]</a></sup>中的分析可能对此有所启发。</p><p>在模型中使用的非线性修正器阈值函数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">h(x) = max\{0,x\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">h</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault">x</span><span class="mclose">}</span></span></span></span>，这让我们模型中的卷积层能与修正线性单元类似（ReLUs）<sup class="footnote-ref"><a href="#fn15" id="fnref15">[15]</a></sup>。算法使用随机梯度下降（SGD），其最小批量为 128，使用动量（momentum）为 0.9，初始步长为 0.01，每 3 个 epoch 减半 10 次。每个 epoch 都采用固定数量的随机训练样本，这些样本在各个类别上使用均匀采样。稍后将详细给出每个数据集使用的样本数。模型使用 Torch 7<sup class="footnote-ref"><a href="#fn16" id="fnref16">[16]</a></sup>实现。</p><h3 id="22-character-quantization"><a class="markdownIt-Anchor" href="#22-character-quantization"></a> 2.2 Character quantization</h3><p>我们的模型接收一个由编码后字符形成的序列作为输入。通过为输入的语言规定字母表（alphabet），然后使用字符的 one-hot 编码来量化每个字符。然后将字符序列转换为这样的序列，一个大小为固定长度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">l_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的向量。序列中超过长度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">l_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>的所有字符都会被忽略，空字符或不在字母表中字符用 0 向量进行量化表示。字符量化的顺序与字符读取的顺序是相反的，因此最新读取的字符始终处于输出的头部，从而使全连接层更易于将权重与最新读取的字符相关联。</p><p>我们所有模型使用的字符表包含 70 个字符，其中包含 26 个英文字母，10 个数字，33 个其他字符和换行符。如下：</p><p><img data-src="alphabet.png" alt=""></p><p>之后，我们还与使用了不同字母表的模型进行对比，在这些模型的字母表中区分大写和小写字母。</p><h3 id="23-model-design"><a class="markdownIt-Anchor" href="#23-model-design"></a> 2.3 Model Design</h3><p>我们设计两个卷积神经网络——一个大的和一个小的。它们都有 9 层，其中 6 层卷积层和 3 层全连接层。见图 Figure 1。</p><p><img data-src="figure-1.png" alt=""></p><p>因为我们字符量化的方法，模型输入的特征（feature）固定为 70，并且输入特征长度（length）为 1014。似乎1014个字符已经可以捕获大多数感兴趣的文本。我们还在 3 个全连接层之间插入两个 dropout 层<sup class="footnote-ref"><a href="#fn17" id="fnref17">[17]</a></sup>来进行正则化。它们的 dropout rate 为 0.5。Table 1列出了卷积层的配置，Table 2列出了全连接层的配置。</p><p><img data-src="table-1.png" alt=""></p><p>我们使用高斯分布来初始化权重。大模型初始化使用的均值和标准差为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.02</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,0.02)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">2</span><span class="mclose">)</span></span></span></span>，小模型初始化使用的均值和标准差为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>0.05</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(0,0.05)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">5</span><span class="mclose">)</span></span></span></span>。</p><p><img data-src="table-2.png" alt=""></p><p>对于不同的问题，模型的输入的长度可能不同（例如我们的模型使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub><mo>=</mo><mn>1014</mn></mrow><annotation encoding="application/x-tex">l_0=1014</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">1</span><span class="mord">4</span></span></span></span>），在我们模型的设计中，很容易知道，给定输入长度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">l_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span>，最后一个卷积层之后（所有全连接层之前）的的输出帧长度为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mn>6</mn></msub><mo>=</mo><mo stretchy="false">(</mo><msub><mi>l</mi><mn>0</mn></msub><mo>−</mo><mn>96</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>27</mn></mrow><annotation encoding="application/x-tex">l_6=(l_0-96)/27</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-.01968em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">9</span><span class="mord">6</span><span class="mclose">)</span><span class="mord">/</span><span class="mord">2</span><span class="mord">7</span></span></span></span>。该数字乘以第 6 层的帧大小将得出第一个全连接层的输入维度。</p><h3 id="24-data-augmentation-using-thesaurus"><a class="markdownIt-Anchor" href="#24-data-augmentation-using-thesaurus"></a> 2.4 Data Augmentation using Thesaurus</h3><p>许多研究人员发现，适当使用数据增强技术对于控制深度学习的泛化错误很有作用。当我们找到模型具有适当的不变属性时，这些技术通常效果很好。就文本而言，使用图像或语言识别中进行信号转换来增强数据是不合理的，因为字符的确切顺序可能会形成严格的句法和语义。因此，进行数据增强的最佳方法是使用人工措词，但这是不现实的且昂贵的，因为我们数据集中的样本量通常非常大。因此，对我们而言，数据增强中最自然的选择是使用同义词来替换单词或短语。</p><p>我们使用英文同义词来进行数据增强实验，该词库是从 LibreOffice 项目中使用的 mytheas 组件中获得的。该词库又是从 Word-Net<sup class="footnote-ref"><a href="#fn18" id="fnref18">[18]</a></sup>中获取的，其中单词或短语的每个同义词都按照与最常见的含义的语义接近度来进行排序。为了确定要替换的单词数，我们从给定的文本中提取所有可替换的单词，然后随机选择 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">r</span></span></span></span> 个要替换的单词来替换。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault" style="margin-right:.02778em">r</span></span></span></span> 的概率由参数为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span></span></span></span> 的几何分布确定，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">[</mo><mi>r</mi><mo stretchy="false">]</mo><mo>∼</mo><msup><mi>p</mi><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">P[r]\thicksim p^r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:.02778em">r</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel amsrm">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.858832em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.664392em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:.02778em">r</span></span></span></span></span></span></span></span></span></span></span>。给定单词的同义次的索引 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathdefault">s</span></span></span></span>也由另一种几何分布来确定，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo stretchy="false">[</mo><mi>s</mi><mo stretchy="false">]</mo><mo>∼</mo><msup><mi>q</mi><mi>s</mi></msup></mrow><annotation encoding="application/x-tex">P[s]\thicksim q^s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathdefault" style="margin-right:.13889em">P</span><span class="mopen">[</span><span class="mord mathdefault">s</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel amsrm">∼</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.858832em;vertical-align:-.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right:.03588em">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.664392em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span></span></span></span></span></span></span></span>。这样，当同义词偏离最常见的含义时，其被选中的可能性就会变小，我们将使用该数据增强技术，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.5</mn><mo separator="true">,</mo><mi>q</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">p=0.5, q=0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8388800000000001em;vertical-align:-.19444em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathdefault" style="margin-right:.03588em">q</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span>。</p><h2 id="3-comparison-models"><a class="markdownIt-Anchor" href="#3-comparison-models"></a> 3. Comparison Models</h2><p>为了公平地比较竞争的模型，我们使用传统和深度学习方法进行了一系列实验。我们尽力选择可以提供可比且具有竞争力的结果的模型，并且在不进行任何模型选择的情况下，如实地报告实验结果。</p><p><strong>Bag-of-words and its TFIDF.</strong> 对于每一个数据集，bag-of-words 模型通过选择训练集中最频繁的 50,000 个单词来构造模型。对于常规的 bag-of-word方法，我们用每个单词的计数作为特征。对于 TFIDF（term-frequency inverse-document-frequency）<sup class="footnote-ref"><a href="#fn19" id="fnref19">[19]</a></sup>版本，我们使用单词的计数作为词频。逆文档频率是样本总数与训练样本子集中样本数的除数的对数。然后通过除以最大特征数值来进行正则化。</p><p><strong>Bag-of-ngrams and its TFIDF.</strong> bag-of-ngrams 模型通过从每一个数据集中的训练集中选择 500,000 个最频繁 n-grams （最多5-grams）来构建。特征的计算方式与 bag-of-words 模型一样。</p><p><strong>Bag-of-means on word embedding.</strong> 我们也有一个实验模型从每个数据集的训练子集的 word2vec<sup class="footnote-ref"><a href="#fn20" id="fnref20">[20]</a></sup>上应用 k 均值，然后将这些学习的方法用作聚类词的代表。我们考虑了在训练子集中出现超过 5 次的所有单词。嵌入的维数为 300。bag-of-means 特征的计算方式与 bag-of-words 模型中的方式一样。均值数为 5000。</p><h3 id="31-deep-learning-methods"><a class="markdownIt-Anchor" href="#31-deep-learning-methods"></a> 3.1 Deep Learning Methods</h3><p>最近，深度学习方法一直开始应用于文本分类中，我们选择两种简单且具有代表性的模型进行比较。其中一种是基于单词的 ConvNets，另一种是简单的长期短期记忆网络模型（LSTM）[^11]。</p><p><strong>Wrod-based ConvNets.</strong> 在有关基于单词的 ConvNets 用于文本分类的大量最新研究中，区别之一是选择使用预训练的或端到端的学习的单词表示形式。我们使用预训练的 word2vec<sup class="footnote-ref"><a href="#fn20" id="fnref20:1">[20:1]</a></sup>嵌入<sup class="footnote-ref"><a href="#fn7" id="fnref7:1">[7:1]</a></sup>和查找表<sup class="footnote-ref"><a href="#fn21" id="fnref21">[21]</a></sup>进行比较。在两种情况下，嵌入大小均为 300，与 bag-of-means 模型相同。为了确保公平地进行比较，每种情况下的模型在层数和每层输出大小方面均与字符级别 ConvNets 相同。还进行了同义词替换数据增强。</p><p><strong>Long-short term memory.</strong> 我们还提供了与 RNN 网络模型（即长期短期记忆网络，LSTM[^11]）的比较。该 LSTM 模型是基于单词的，与上文的模型一样，使用大小为 300 的预训练 word2vec 嵌入。通过取所有的 LSTM 单元的输出平均值形成一个特征向量，然后对该特征向量使用多项逻辑回归来构建模型。模型输出的维度为 512。本文使用的 LSTM 变体是常见的 “vanilla” 结构<sup class="footnote-ref"><a href="#fn22" id="fnref22">[22]</a></sup><sup class="footnote-ref"><a href="#fn23" id="fnref23">[23]</a></sup>。我们还使用了梯度正则，取值为 5。见 Figure 2。</p><p><img data-src="figure-2.png" alt=""></p><h3 id="32-choice-of-alphabet"><a class="markdownIt-Anchor" href="#32-choice-of-alphabet"></a> 3.2 Choice of Alphabet</h3><p>对于英文字母表，一个明显的选择数是否区分大小写字母。本文给出了选择的相关实验结果，结果表明，当考虑区分大小写时，通常（但并非总是）会得到比较差的结果。一种可能的解释是语义不会随字母大小写发生改变，因此正则化必然是存在好处的。</p><h2 id="4-large-scale-datasets-and-results"><a class="markdownIt-Anchor" href="#4-large-scale-datasets-and-results"></a> 4. Large-scale Datasets and Results</h2><p>以前在不同领域对 ConvNets 进行的研究表明，它们通常与大规模的数据集配合进行训练。特别是当模型使用低级原始特征表示的时候（本文中使用的字符级别）。然而，大多数用于文本分类的开放数据集都非常小，并且大规模的数据集通常被划分成较小的训练集以及测试集<sup class="footnote-ref"><a href="#fn24" id="fnref24">[24]</a></sup>。因此，我们没有使用它们，而自己构建几个大规模数据集，范围从数十万到数百万个样本，见 Table 3。</p><p><img data-src="table-3.png" alt=""></p><p><strong>AG’s new corpus.</strong>（AG 新闻语料）我们通过网络<sup class="footnote-ref"><a href="#fn13" id="fnref13:1">[13:1]</a></sup>获取了 AG 新闻文章语料库。它包含来自 2000 多个新闻来源的 496,835 个分类的新闻文章。我们仅从标题和描述字段中选取该语料库中最多的 4 个类别来构建数据集。每个类别的训练样本数据量为 30,000，测试数量为 1900。</p><p><strong>Sogou’s new corpus.</strong> 该数据集是 SogouuCA 和 SogouCS 新闻语料库的组合<sup class="footnote-ref"><a href="#fn25" id="fnref25">[25]</a></sup>，总共包含各种主题中的 2,909,551 调新闻文章。然后，通过手动分类新闻的领域名，我们使用新闻的 URL 标记新闻。这使我们获得了大量带有其类别标签的新闻文章。类别很多，但大多数包含很少的文章。我们选择 5 个类别-“体育”，“金融”，“娱乐”，“汽车”和“技术”。每个班级选择的训练样本数量为 90,000，测试集为 12,000。尽管这是中文数据集，我们使用 pypinyin 包集合 jieba 分词来生成拼音。然后将英文模型应用于此数据集则无需进行任何更改。其中使用标题和内容字段。</p><p><img data-src="table-4.png" alt=""></p><p><strong>DBPedia ontology dataset.</strong> DBpedia 是一个众包社区，旨在从 Wikipedia 中提取结构化信息<sup class="footnote-ref"><a href="#fn26" id="fnref26">[26]</a></sup>。通过从 DBPedia 2014中选择 14 个不重叠的类来构建 DBPedia 本体数据集。从这 14 个类别中的每一个类中，我们随机选择 40,000 个训练样本和 5,000 个测试样本。我们用于此数据集的字段包含每个 Wikipedia 文章的标题和摘要。</p><p><strong>Yelp reviews.</strong> Yelp 评论数据集来自 2015 年的 Yelp 数据集挑战赛。该数据集包含 1,569,264 个具有评论文本的样本。从该数据集构建了两个分类任务——一个预测用户给定的恒星总数，另一个通过考虑恒星 1 和 2 为负，恒星 3 和 4 正，来预测极性标签。完整的数据集在每颗恒星中具有 130,000 个训练样本和 10,000 个测试样本，而极性数据集在每个极性中具有 280,000 个训练样本和 19,000 个测试样本。</p><p><strong>Amazon reviews.</strong> 我们从斯坦福网络分析项目 （SNAP）获得了 Amazon 评论数据集，该数据集横跨 18 年，来自 2,643,053 个产品的 6,43,669 个用户中的 34,686,770 条评论<sup class="footnote-ref"><a href="#fn27" id="fnref27">[27]</a></sup>。与 Yelp 评论数据集相似，我们也构建了 2 个数据集——一个完整分数预测和另一个极性预测。完整的数据集包含每个类别中的 600,000 个训练样本和 130,000 个测试样本，而极性数据集则包含 1,800,000 个训练样本和 200,00 个测试样本。</p><p>Table 4列出了我们从这些数据集中获得的所有适用与模型的测试错误。请注意，由于我们没有中文词库，因此使用搜狗数据集并没有使用数据增强。我们将最佳结果标记为蓝色，将较差结果标记为红色。</p><h2 id="5-discussion"><a class="markdownIt-Anchor" href="#5-discussion"></a> 5. Discussion</h2><p><img data-src="figure-3.png" alt=""></p><p>为了更好地了解 Table 4 中的结果，我们在本节中提供了一些经验分析。为了方便我们的分析，我们在Figure 3 中提供了相对于比较模型的相对误差。</p><p>这些图的每一个都是通过比较模型和字符级别 ConvNets 模型的误差之间的差来计算的，然后除以比较模型误差。图中的所有 ConvNets 都是带有词库数据增强的大型模型。</p><p><strong>字符级别 ConvNets 是一个高效的方法</strong>。我们实验中的最重要的结论是，字符级别的 ConvNets 可以不需要单词的情况下进行文本分析。这有力地证明了，语言也可以被视为与任何其他种类类似的信号。</p><p>**数据集大小在传统模型与 ConvNets 模型之间通常差异明显。**从 Figure 3 中可以明显看出，较大的数据集往往表现更好。像 n-grams TFIDF 这样的传统方法荏苒是多达几十万个样本规模数据集的强大候选者，而字符级别 ConvNets 只有在数据集规模到达几百万个样本规模时，我们才能看到它较好的表现。</p><p>**ConvNets 可以很好地处理用户生成的数据。**用户生成数据的文字编排程度不同。例如，在我们的百万个数据集岁，亚马逊评论数据集中往往是原始的用户输入，而用户可能会格外注意他们在 Yahoo! 上的写的评论回答。比较基于单词的深度模型（Figure 3, c,d,e）的图表表明，字符级别 ConvNets 可以更好地处理较少编排的用户生成的文本。此属性表明 ConvNets 可以对现实情况具有更好的适用性。但是，需要进一步的分析来验证 ConvNets 确实擅长于识别额外的字符组合（例如拼写错误和表情符号）的假设，因为仅我们的实验并未显示任何明确的证据。</p><p><strong>根据字母表的选择有会不同的结果。</strong> Figure 3表明，使用区分字母大小写的字母表，可能会有不同的结果。对于百万规模的数据集，通常不做这种区分会更好。一种可能的解释是存在正则化效应，但这是有待验证的。</p><p>**任务的语义可能并不重要。**我们的数据集包括两种任务：情感分析（Yelp 和 Amazon 评论）和主题分类（所有其他任务）。任务语义上的这种二分法似乎在决定哪种方法更好时不起作用。</p><p>**Bag-of-means 是对 word2vec 的滥用<sup class="footnote-ref"><a href="#fn28" id="fnref28">[28]</a></sup>。**从 Table 4 和 Figure 3a 中可以观察到最明显的事实之一是，bag-of-means 模型在每种情况下的性能较差。与传统模型相比，这表明分布式单词表示的这种简单使用可能无法为我们提供文本分类的优势。但是，我们的实验并不代表任何其他语言处理任务或以任何其他方式使用 word2vec。</p><p>**天下没有免费的午餐。**我们的实验再次验证了没有一个适用于所有数据集的机器学习模型。本节中讨论的因素都可以在决定哪种方法最合适某些特定应用的过程中发挥作用。</p><h2 id="6-conclusion-and-outlook"><a class="markdownIt-Anchor" href="#6-conclusion-and-outlook"></a> 6. Conclusion and Outlook</h2><p>本文提供了针对文本分类的字符级别分类卷积网络的实证研究。我们使用了几个大型数据集，并使用许多传统模型和深度学习模型进行了比较。一方面，分析表明字符级别的 ConvNet 是一种有效的方法。另一方面，我们的模型在比较实验中的表现效果取决于许多因素，例如数据集的大小，文本是否经过整理以及字母表的选择。</p><p>将来，我们希望将字符级别的 ConvNets 应用到更广泛的语言处理任务中，尤其是在需要结构化输出的应用场景中。</p><h2 id="acknowledgement"><a class="markdownIt-Anchor" href="#acknowledgement"></a> Acknowledgement</h2><p>我们衷心感谢 NVIDIA Corporation 的捐赠，其中包括捐赠了2 个用于该研究的 Tesla K40 GPU。 我们非常感谢 <a target="_blank" rel="noopener" href="http://Amazon.com">Amazon.com</a> Inc 支持用于这项研究的 AWS in Education Research 补助金。</p><h2 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h2><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>T. Joachims. Text categorization with suport vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, pages 137–142. Springer-Verlag, 1998. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li><li id="fn2" class="footnote-item"><p>Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, Winter 1989. <a href="#fnref2" class="footnote-backref">↩︎</a></p></li><li id="fn3" class="footnote-item"><p>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November 1998. <a href="#fnref3" class="footnote-backref">↩︎</a></p></li><li id="fn4" class="footnote-item"><p>L. Bottou, F. Fogelman Soulié, P. Blanchet, and J. Lienard. Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition. In Proceedings of EuroSpeech 89, volume 2, pages 537–540, Paris, France, 1989. <a href="#fnref4" class="footnote-backref">↩︎</a></p></li><li id="fn5" class="footnote-item"><p>A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):328–339, 1989. <a href="#fnref5" class="footnote-backref">↩︎</a></p></li><li id="fn6" class="footnote-item"><p>C. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 69–78, Dublin, Ireland, August 2014. Dublin City University and Association for Computational Linguistics. <a href="#fnref6" class="footnote-backref">↩︎</a></p></li><li id="fn7" class="footnote-item"><p>Y. Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar, October 2014. Association for Computational Linguistics. <a href="#fnref7" class="footnote-backref">↩︎</a> <a href="#fnref7:1" class="footnote-backref">↩︎</a></p></li><li id="fn8" class="footnote-item"><p>R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional neural networks. CoRR, abs/1412.1058, 2014. <a href="#fnref8" class="footnote-backref">↩︎</a></p></li><li id="fn9" class="footnote-item"><p>I. Kanaris, K. Kanaris, I. Houvardas, and E. Stamatatos. Words versus character n-grams for anti-spam filtering. International Journal on Artificial Intelligence Tools, 16(06):1047–1067, 2007. <a href="#fnref9" class="footnote-backref">↩︎</a></p></li><li id="fn10" class="footnote-item"><p>C. D. Santos and B. Zadrozny. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818–1826, 2014. <a href="#fnref10" class="footnote-backref">↩︎</a> <a href="#fnref10:1" class="footnote-backref">↩︎</a></p></li><li id="fn11" class="footnote-item"><p>Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101–110. ACM, 2014. <a href="#fnref11" class="footnote-backref">↩︎</a> <a href="#fnref11:1" class="footnote-backref">↩︎</a></p></li><li id="fn12" class="footnote-item"><p>D. Rumelhart, G. Hintont, and R. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533–536, 1986. <a href="#fnref12" class="footnote-backref">↩︎</a></p></li><li id="fn13" class="footnote-item"><p>Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2559–2566. IEEE, 2010. <a href="#fnref13" class="footnote-backref">↩︎</a> <a href="#fnref13:1" class="footnote-backref">↩︎</a></p></li><li id="fn14" class="footnote-item"><p>Y.-L. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 111–118, 2010. <a href="#fnref14" class="footnote-backref">↩︎</a></p></li><li id="fn15" class="footnote-item"><p>V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010. <a href="#fnref15" class="footnote-backref">↩︎</a></p></li><li id="fn16" class="footnote-item"><p>R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011. <a href="#fnref16" class="footnote-backref">↩︎</a></p></li><li id="fn17" class="footnote-item"><p>G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012. <a href="#fnref17" class="footnote-backref">↩︎</a></p></li><li id="fn18" class="footnote-item"><p>C. Fellbaum. Wordnet and wordnets. In K. Brown, editor, Encyclopedia of Language and Linguistics, pages 665–670, Oxford, 2005. Elsevier. <a href="#fnref18" class="footnote-backref">↩︎</a></p></li><li id="fn19" class="footnote-item"><p>K. S. Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21, 1972. <a href="#fnref19" class="footnote-backref">↩︎</a></p></li><li id="fn20" class="footnote-item"><p>T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. 2013. <a href="#fnref20" class="footnote-backref">↩︎</a> <a href="#fnref20:1" class="footnote-backref">↩︎</a></p></li><li id="fn21" class="footnote-item"><p>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, Nov. 2011. <a href="#fnref21" class="footnote-backref">↩︎</a></p></li><li id="fn22" class="footnote-item"><p>A. Graves and J. Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602–610, 2005. <a href="#fnref22" class="footnote-backref">↩︎</a></p></li><li id="fn23" class="footnote-item"><p>K. Greff, R. K. Srivastava, J. Koutnı́k, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015. <a href="#fnref23" class="footnote-backref">↩︎</a></p></li><li id="fn24" class="footnote-item"><p>D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5:361–397, 2004. <a href="#fnref24" class="footnote-backref">↩︎</a></p></li><li id="fn25" class="footnote-item"><p>C. Wang, M. Zhang, S. Ma, and L. Ru. Automatic online news issue construction in web environment. In Proceedings of the 17th International Conference on World Wide Web, WWW ’08, pages 457–466, New York, NY, USA, 2008. ACM. <a href="#fnref25" class="footnote-backref">↩︎</a></p></li><li id="fn26" class="footnote-item"><p>J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal, 2014. <a href="#fnref26" class="footnote-backref">↩︎</a></p></li><li id="fn27" class="footnote-item"><p>J. McAuley and J. Leskovec. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13, pages 165–172, New York, NY, USA, 2013. ACM. <a href="#fnref27" class="footnote-backref">↩︎</a></p></li><li id="fn28" class="footnote-item"><p>G. Lev, B. Klein, and L. Wolf. In defense of word embedding for generic text representation. In C. Biemann, S. Handschuh, A. Freitas, F. Meziane, and E. Mtais, editors, Natural Language Processing and Information Systems, volume 9103 of Lecture Notes in Computer Science, pages 35–50. Springer International Publishing, 2015. <a href="#fnref28" class="footnote-backref">↩︎</a></p></li></ol></section></div><footer class="post-footer"><div class="post-copyright"><ul><li class="post-copyright-author"><strong>Written by: </strong>Xiang Zhang et al.</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://rovo98.github.io/posts/1b531d86/" title="Character-level Convolutional Networks for Text Classification">http://rovo98.github.io/posts/1b531d86/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li></ul></div><div class="post-tags"><a href="/tags/paper-translation/" rel="tag"><i class="fa fa-tag"></i> paper translation</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/ea5d9796/" rel="prev" title="A Compact Encoding for Efficient Character-level Deep Text Classification"><i class="fa fa-chevron-left"></i> A Compact Encoding for Efficient Character-level Deep Text Classification</a></div><div class="post-nav-item"><a href="/posts/c598fb85/" rel="next" title="A Polynomial Algorithm for Testing Diagnosability of Discrete-Event Systems.">A Polynomial Algorithm for Testing Diagnosability of Discrete-Event Systems. <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">rovo98</span></div><div class="busuanzi-count"><span class="post-meta-item" id="busuanzi_container_site_uv"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-item" id="busuanzi_container_site_pv"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div><div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script><script src="/js/third-party/search/local-search.js"></script><script src="/js/third-party/fancybox.js"></script><script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"jUwzFlXSJ71zpvuEefX2F1aD-gzGzoHsz","app_key":"duLPBvpLL5Q5WAnwRHnBDaR5","server_url":"https://juwzflxs.lc-cn-n1-shared.com","security":false}</script><script src="/js/third-party/statistics/lean-analytics.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.16/katex.min.css" integrity="sha256-e8fkE+LP8R8PMyAaC6jji0vzbcvXB0WnNxM/19zvcdU=" crossorigin="anonymous"><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":100,"height":180},"mobile":{"show":false},"log":false,"tagMode":false});</script></body></html>